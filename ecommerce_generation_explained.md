# E-commerce CSV Data Generation Explained

This document provides a detailed explanation of how the e-commerce CSV data is generated and written by the Gengo data generation tool. It also discusses potential improvements and speed-ups to the process.

## Data Generation Flow

The e-commerce data generation process can be broken down into the following steps:

### 1. User Input and Sizing

1.  **User Input:** The process starts in `main.go` which calls `core.GetUserInputForModel()` in `internal/core/input.go`. This function prompts the user for the following information:
    *   Data model to generate (e-commerce, financial, or medical).
    *   Approximate target size in GB.
    *   Desired output format (CSV, JSON, or Parquet).
    *   Output directory name.

2.  **Row Count Estimation:** Once the user provides the target size, the `CalculateECommerceRowCounts` function in `internal/core/sizing.go` is called. This function estimates the number of rows for each table in the e-commerce data model based on the target size. It uses a constant `effectiveSizePerOrderItem` to estimate the total number of order items and then derives the other table sizes based on predefined ratios.

### 2. Data Generation

The data generation is orchestrated by the `generateECommerceDataConcurrently` function in `internal/core/orchestrator.go`. This function uses a concurrent approach to generate the data for the different tables.

*   **Dimension Tables:** The dimension tables (`customers`, `customer_addresses`, `suppliers`, `products`, and `product_categories`) are generated concurrently in separate goroutines.
    *   `GenerateCustomers` in `internal/simulation/ecommerce/simulate_dims.go`: Generates customer data using the `gofakeit` library.
    *   `GenerateCustomerAddresses` in `internal/simulation/ecommerce/simulate_dims.go`: Generates customer address data, with each customer having between 1 and 3 addresses.
    *   `GenerateSuppliers` in `internal/simulation/ecommerce/simulate_dims.go`: Generates supplier data.
    *   `GenerateProductCategories` in `internal/simulation/ecommerce/simulate_dims.go`: Generates a fixed list of 10 product categories.
    *   `GenerateProducts` in `internal/simulation/ecommerce/simulate_dims.go`: Generates product data, with each product being assigned a random supplier and category.

*   **Fact Tables:** The fact tables (`fact_orders_header` and `fact_order_items`) are generated by the `GenerateECommerceModelData` function in `internal/simulation/ecommerce/simulate_facts.go`. This function also uses a concurrent approach, splitting the work of generating orders and order items across multiple goroutines.
    *   **Weighted Samplers:** To make the data more realistic, weighted samplers are used to select customers and products for orders. This ensures that some customers and products appear more frequently than others.
    *   **Order Generation:** For each order, a random number of items (between 1 and 10) is generated. The `unit_price` is taken from the product's `base_price`, and a discount is randomly applied.

### 3. Data Writing

Once the data for a table is generated in memory, it is written to a CSV file.

*   **Concurrent Writing:** The `generateECommerceDataConcurrently` function in `internal/core/orchestrator.go` writes the dimension tables to CSV files concurrently as they are generated. The fact tables are also written concurrently.
*   **CSV Writing Logic:** The actual writing to the CSV file is handled by the `writeSliceToCSV` function in `internal/formats/csv.go`. This function takes a slice of structs, extracts the header information from the struct tags (`json` or `parquet`), and then writes the data to the CSV file row by row.

## Potential Improvements and Speed-ups

### Memory Usage

*   **Problem:** The current approach generates all the data for a table in memory before writing it to a file. This can lead to high memory usage, especially for large datasets.
*   **Improvement:** Instead of generating all the data at once, the data could be generated in chunks and written to the file incrementally. This would significantly reduce the memory footprint of the application. For example, the `generateECommerceFactsChunk` function could write the generated headers and items to a file directly instead of returning them.

### Concurrency

*   **Problem:** The current concurrency model is effective, but there are some areas for improvement. For example, the fact table generation only starts after all the dimension tables have been generated.
*   **Improvement:** The fact table generation could start as soon as the necessary dimension data is available. For example, the `GenerateECommerceModelData` function could be called as soon as the `customers`, `customer_addresses`, and `products` tables have been generated. This would allow the dimension and fact table generation to overlap more, potentially reducing the total generation time.

### Data Generation Logic

*   **Problem:** The data generation logic is good, but it could be made more realistic. For example, the product prices are generated using a normal distribution, but real-world product prices often follow a different distribution.
*   **Improvement:** The data generation logic could be improved by using more realistic data distributions and by adding more complex relationships between the different tables. For example, the `product_category` could influence the `base_price` of a product.

### File Writing

*   **Problem:** The `writeSliceToCSV` function uses reflection to get the header and field information from the structs. While this is a flexible approach, it can be slower than a more direct approach.
*   **Improvement:** For performance-critical applications, you could write a dedicated CSV writing function for each struct type. This would avoid the overhead of reflection and could lead to a significant speed-up in the file writing process.

## Conclusion

The Gengo e-commerce data generation tool is a powerful and flexible tool for creating large volumes of realistic data. The current implementation is well-structured and uses concurrency effectively to speed up the generation process. However, there are several areas where the tool could be improved, particularly in terms of memory usage and data generation logic. By implementing the suggestions in this document, the tool could be made even more efficient and powerful.
